{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hdqn_corridor.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1qq_KqjzvEV2sSxLoxNRpjlAZwJll6sb9","authorship_tag":"ABX9TyOfxfYq4Kw5xTDipDVDpOg+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"i0c3nCkJ-pRv"},"source":["import tensorflow as tf\n","import numpy as np\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aqRIeFFayfO_"},"source":["class Corridor:\n"," \n","    def reset(self):\n","        self.current_state = 3\n","        self.steps = 0\n","        self.s6_visits = 0\n","        return self.vectorize(self.current_state)\n","\n","    def step(self, action):\n","        # Left\n","        if action == 0:\n","            self.current_state -= 1\n","        # Right\n","        if action == 1 and self.current_state != 6:\n","            # Move from state 5 to 6\n","            if self.current_state == 5:\n","                self.s6_visits += 1\n","            self.current_state += 1\n","        self.steps += 1\n","\n","        if self.current_state == 0:\n","            if self.s6_visits >= 2:\n","                return self.vectorize(self.current_state), 1.0, True\n","            return self.vectorize(self.current_state), 0.01, True\n","\n","        if self.steps == 20:\n","            return self.vectorize(self.current_state), 0.0, True\n","\n","        return self.vectorize(self.current_state), 0.0, False\n","    \n","    def vectorize(self, state):\n","        vector = np.zeros(7)\n","        vector[state] = 1\n","        return vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b-v4h6O1yfed"},"source":["class HDQN:\n","\n","    def __init__(self):\n","        self.memory = []\n","        self.epsilon = 1.0\n","        self.epsilon_decay = 0.9997\n","        self.batch_size = 64\n","        self.discount_rate = 0.99\n","        self.learning_rate = 0.001\n","        self.tau = 0.001\n","        self.model = self.build_model()\n","        self.compile_model(self.model)\n","        self.target_model = self.build_model()\n","        self.target_model.set_weights(self.model.get_weights())\n","\n","    def build_model(self):\n","        model = tf.keras.Sequential()\n","        model.add(tf.keras.layers.Dense(16, input_shape=(7, ), activation='relu'))\n","        model.add(tf.keras.layers.Dense(32, activation='relu'))\n","        model.add(tf.keras.layers.Dense(7, activation='linear'))\n","        return model\n","\n","    def compile_model(self, model):\n","        model.compile(loss=\"huber_loss\",\n","                    optimizer=tf.keras.optimizers.RMSprop(lr=self.learning_rate, clipnorm=1.0))\n","        model.summary()\n","\n","    def select_option(self, state):\n","        if np.random.rand() < self.epsilon:\n","            applicable = np.delete(np.arange(7), np.argmax(state))\n","            return np.random.choice(applicable)\n","        state = np.reshape(state, (1, 7))\n","        pred = self.model.predict(state)[0]\n","        pred[np.argmax(state[0])] = np.NINF\n","        return np.argmax(pred)\n","\n","    def store_transition(self, state, option, reward, next_state, done):\n","        self.memory.append((state, option, reward, next_state, done))\n","        if len(self.memory) > 100000:\n","            self.memory = self.memory[-100000:]\n","\n","    def replay(self):\n","        # state, option, reward, next_state, done\n","        batch = random.sample(self.memory, self.batch_size)\n","\n","        x = [transition[0] for transition in batch]\n","        x = np.reshape(x, (self.batch_size, 7))\n","        y = self.model.predict(x)\n","        next_x = [transition[3] for transition in batch]\n","        next_x = np.reshape(next_x, (self.batch_size, 7))\n","        next_y = self.target_model.predict(next_x)\n","\n","        for i, transition in enumerate(batch):\n","            option = transition[1]\n","            reward = transition[2]\n","            done = transition[4]\n","            if done:\n","                y[i, option] = reward\n","            else:\n","                y[i, option] = reward + self.discount_rate * np.amax(next_y[i])\n","            \n","        self.model.fit(x, y, epochs=1, verbose=0)\n","        self.update_target_model()\n","\n","    def update_target_model(self):\n","        model_weights = self.model.get_weights()\n","        target_weights = self.target_model.get_weights()\n","        for i in range(len(model_weights)):\n","            target_weights[i] = target_weights[i] + self.tau * (model_weights[i] - target_weights[i])\n","        self.target_model.set_weights(target_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uidKwSy9yfuz"},"source":["def select_action(state, option):\n","    return 1 if np.argmax(state) < option else 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KCHL_b5_yf5e"},"source":["def train(start, runs):\n","    if start == 0:\n","        rewards = np.zeros((runs, 10000))\n","        terminal_steps = np.zeros((runs, 10000))\n","    else:\n","        rewards = np.load(f\"./drive/My Drive/corridor/hdqn_rewards.npy\")\n","        terminal_steps = np.load(f\"./drive/My Drive/corridor/hdqn_steps.npy\")\n","   \n","    for run in range(start, runs):\n","        print(\"\\nRun \" + str(run))\n","\n","        env = Corridor()\n","        meta = HDQN()\n","        steps = 0\n","\n","        for episode in range(10000):\n","            # print(\"\\nEpisode \" + str(episode))\n","\n","            done = False\n","            meta_state = env.reset()\n","            episode_reward = 0\n","\n","            while not done:\n","                option = meta.select_option(meta_state)\n","                reached = np.argmax(meta_state) == option\n","                \n","                steps += 1\n","                option_reward = 0\n","\n","                state = meta_state\n","                while not done and not reached:\n","                    action = select_action(state, option)\n","                    next_state, reward, done = env.step(action)\n","                    \n","                    option_reward += reward\n","                    episode_reward += reward\n","                    \n","                    reached = np.argmax(next_state) == option\n","                    state = next_state\n","\n","                meta.store_transition(meta_state, option, option_reward, state, done)\n","                meta_state = state\n","\n","                if len(meta.memory) >= 2000:\n","                    meta.replay()\n","                    if meta.epsilon > 0.01:\n","                        meta.epsilon *= meta.epsilon_decay\n","\n","            rewards[run, episode] = episode_reward\n","            terminal_steps[run, episode] = steps\n","\n","            # print(\"reward =  \" + str(episode_reward))\n","            # print(\"terminal step =  \" + str(steps))\n","            # print(\"epsilon = \" + str(meta.epsilon))\n","\n","            if episode % 100 == 99:\n","                np.save(f\"./drive/My Drive/corridor/hdqn_rewards\", rewards)\n","                np.save(f\"./drive/My Drive/corridor/hdqn_steps\", terminal_steps)\n","        \n","        meta.model.save(f\"./drive/My Drive/corridor/hdqn_{run}_{episode}.h5\")\n","        meta.target_model.save(f\"./drive/My Drive/corridor/hdqn_target_{run}_{episode}.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"luc4SQfTib6N"},"source":["train(0, 10)"],"execution_count":null,"outputs":[]}]}