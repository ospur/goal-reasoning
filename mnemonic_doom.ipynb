{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnemonic_doom.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1kCBu1lzMlD9fj6l2JaYsteHc_zQFEm5Q","authorship_tag":"ABX9TyOaWLr5+0wS0unxp+pT6MYf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"4oyvO0xK-ZCH"},"source":["%%bash\n","\n","apt-get update\n","\n","# ZDoom dependencies\n","apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n","nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n","libopenal-dev timidity libwildmidi-dev unzip\n","\n","# Boost libraries\n","apt-get install libboost-all-dev\n","\n","# Lua binding dependencies\n","apt-get install liblua5.1-dev\n","\n","pip install git+https://github.com/mwydmuch/ViZDoom"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ifD0jhIh_GLs"},"source":["import tensorflow as tf\n","import numpy as np\n","import cv2\n","import vizdoom"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8rTNnC2d_HSQ"},"source":["env = vizdoom.DoomGame()\n","env.set_doom_scenario_path(f\"./drive/My Drive/doom/corridor.wad\")\n","env.set_doom_map(\"map01\")\n","\n","env.set_screen_resolution(vizdoom.ScreenResolution.RES_320X240)\n","env.set_screen_format(vizdoom.ScreenFormat.RGB24)\n","\n","env.set_render_hud(False)\n","env.set_render_minimal_hud(False)\n","env.set_render_crosshair(False)\n","env.set_render_weapon(True)\n","env.set_render_messages(False)\n","env.set_render_screen_flashes(True)\n","\n","env.add_available_button(vizdoom.Button.MOVE_BACKWARD)\n","env.add_available_button(vizdoom.Button.MOVE_FORWARD)\n","\n","env.add_available_game_variable(vizdoom.GameVariable.POSITION_X)\n","\n","env.set_episode_timeout(200)\n","\n","env.set_window_visible(False)\n","env.set_mode(vizdoom.Mode.PLAYER)\n","\n","# #env.set_console_enabled(True)\n","\n","env.init()\n","\n","actions = [[1, 0], [0, 1]]\n","positions = [32, 96, 160, 224, 288, 352, 416]\n","\n","def select_action(x, option):\n","    return 1 if x < positions[option] else 0\n","\n","def option_reached(x, option):\n","    return True if abs(x - positions[option]) < 5 else False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KRz8GvQT_WPV"},"source":["class RecurrentHReinforce:\n","\n","    def __init__(self):\n","        self.memory = []\n","        self.frames = []\n","        self.options = []\n","        self.rewards = []\n","        self.discount_rate = 0.99\n","        self.learning_rate = 0.001\n","        self.model = self.build_model()\n","\n","    def build_model(self):\n","        model = tf.keras.Sequential()\n","        model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(\n","            32, (8, 8), strides=(4, 4), activation='relu'), input_shape=(None, 100, 100, 3)))\n","        model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(\n","            64, (4, 4), strides=(2, 2), activation='relu')))\n","        model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten()))\n","        model.add(tf.keras.layers.GRU(256, return_sequences=True))\n","        model.add(tf.keras.layers.Dense(7, activation='softmax'))\n","\n","        model.compile(loss=\"categorical_crossentropy\",\n","                      optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate, clipnorm=1.0))\n","        model.summary()\n","\n","        return model\n","\n","    def select_option(self, frame_list, x):\n","        applicable = np.delete(np.arange(7), x // 64)\n","        frame_list = np.reshape(frame_list, (1, len(frame_list), 100, 100, 3)) / 255\n","        prob = np.delete(self.model.predict(frame_list)[0, -1], x // 64)\n","        if np.sum(prob) == 0:\n","            return np.random.choice(applicable)\n","        prob /= np.sum(prob)\n","        return np.random.choice(applicable, 1, p=prob)[0]\n","\n","    def store_transition(self, frame, option, reward):\n","        self.frames.append(frame)\n","        self.options.append(option)\n","        self.rewards.append(reward)\n","\n","    def store_episode(self):\n","        self.memory.append((self.frames, self.options, self.rewards))\n","        self.frames = []\n","        self.options = []\n","        self.rewards = []\n","\n","    def update(self, episode):\n","        trajectory = self.memory[episode]\n","        frames = trajectory[0][:-1]\n","        options = trajectory[1][:-1]\n","        rewards = trajectory[2][:-1]\n","\n","        T = len(frames)\n","\n","        returns = np.zeros(T)\n","        returns[-1] = rewards[-1]\n","        for t in reversed(range(1, T)):\n","            returns[t - 1] = rewards[t - 1] + self.discount_rate * returns[t]\n","\n","        x = np.reshape(frames, (1, T, 100, 100, 3)) / 255\n","        y = np.zeros((1, T, 7))\n","\n","        for i in range(T):\n","            y[0, i, options[i]] = returns[i]\n","\n","        self.model.fit(x, y, epochs=1, verbose=0)\n","\n","    def batch_update(self, start, end):\n","        for episode in range(start, end):\n","            self.update(episode)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jf82ttYW_j_X"},"source":["def train(start, runs):\n","    if start == 0:\n","        rewards = np.zeros((runs, 10000))\n","    else:\n","        rewards = np.load(f\"./drive/My Drive/doom/mnemonic_rewards.npy\")\n","\n","    for run in range(start, runs):\n","        print(\"\\nRun \" + str(run))\n","\n","        meta = RecurrentHReinforce()\n","\n","        for episode in range(10000):\n","            env.new_episode()\n","            meta_state = env.get_state()\n","            episode_reward = 0\n","\n","            frame_list = []\n","            frame = cv2.resize(meta_state.screen_buffer, (100, 100))\n","            frame_list.append(frame)\n","\n","            while not env.is_episode_finished():\n","                if episode < 1000:\n","                    # applicable = np.delete(np.arange(7), meta_state.game_variables[0] // 64)\n","                    option = np.random.choice(7)\n","                else:\n","                    option = meta.select_option(frame_list, meta_state.game_variables[0])\n","                reached = option_reached(meta_state.game_variables[0], option)\n","\n","                option_reward = 0\n","\n","                state = meta_state\n","                while not env.is_episode_finished() and not reached:\n","                    action = select_action(state.game_variables[0], option)\n","                    reward = env.make_action(actions[action])\n","                    next_state = env.get_state()\n","                    \n","                    option_reward += reward\n","                    episode_reward += reward\n","                    if next_state:\n","                        reached = option_reached(next_state.game_variables[0], option)\n","                        state = next_state\n","\n","                meta.store_transition(frame, option, option_reward)\n","                meta_state = state\n","                frame = cv2.resize(meta_state.screen_buffer, (100, 100))\n","                frame_list.append(frame)\n","\n","            meta.store_episode()\n","            meta.update(episode)\n","\n","            rewards[run, episode] = episode_reward\n","\n","            if episode % 1000 == 999:\n","                np.save(f\"./drive/My Drive/doom/mnemonic_rewards\", rewards)\n","\n","        meta.model.save(f\"./drive/My Drive/doom/mnemonic_{run}_{episode}.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AuQWvGPE_3dC"},"source":["train(0, 10)"],"execution_count":null,"outputs":[]}]}