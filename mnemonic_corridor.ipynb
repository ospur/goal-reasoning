{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnemonic_corridor.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1D72k1VHHZxLOZ-WpxGOeZb3vsNcV1yGC","authorship_tag":"ABX9TyMO/JeuY2rbVuSY5pZzBzoZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Fe0lxlGzOAbZ"},"source":["import tensorflow as tf\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"199Q4sqiHHQp"},"source":["class Corridor:\n"," \n","    def reset(self):\n","        self.current_state = 3\n","        self.steps = 0\n","        self.s6_visits = 0\n","        return self.vectorize(self.current_state)\n","\n","    def step(self, action):\n","        # Left\n","        if action == 0:\n","            self.current_state -= 1\n","        # Right\n","        if action == 1 and self.current_state != 6:\n","            # Move from state 5 to 6\n","            if self.current_state == 5:\n","                self.s6_visits += 1\n","            self.current_state += 1\n","        self.steps += 1\n","\n","        if self.current_state == 0:\n","            if self.s6_visits >= 2:\n","                return self.vectorize(self.current_state), 1.0, True\n","            return self.vectorize(self.current_state), 0.01, True\n","\n","        if self.steps == 20:\n","            return self.vectorize(self.current_state), 0.0, True\n","\n","        return self.vectorize(self.current_state), 0.0, False\n","    \n","    def vectorize(self, state):\n","        vector = np.zeros(7)\n","        vector[state] = 1\n","        return vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5TbckHlKHPLC"},"source":["class RecurrentHReinforce:\n","\n","    def __init__(self, new=True):\n","        self.memory = []\n","        self.states = []\n","        self.options = []\n","        self.rewards = []\n","        self.discount_rate = 0.99\n","        self.learning_rate = 0.001\n","        self.model = self.build_model()\n","\n","    def build_model(self):\n","        model = tf.keras.Sequential()\n","        model.add(tf.keras.layers.GRU(64, input_shape=(None, 7), return_sequences=True))\n","        model.add(tf.keras.layers.Dense(7, activation='softmax'))\n","        model.compile(loss=\"categorical_crossentropy\",\n","                    optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate, clipnorm=1.0))\n","        model.summary()\n","        return model\n","\n","    def select_option(self, state_list):\n","        applicable = np.delete(np.arange(7), np.argmax(state_list[-1]))\n","        state_list = np.reshape(state_list, (1, len(state_list), 7))\n","        prob = np.delete(self.model.predict(state_list)[0, -1], np.argmax(state_list[0, -1]))\n","        if np.sum(prob) == 0:\n","            return np.random.choice(applicable)\n","        prob /= np.sum(prob)\n","        return np.random.choice(applicable, 1, p=prob)[0]\n","\n","    def store_transition(self, state, option, reward):\n","        self.states.append(state)\n","        self.options.append(option)\n","        self.rewards.append(reward)\n","\n","    def store_episode(self):\n","        self.memory.append((self.states, self.options, self.rewards))\n","        self.states = []\n","        self.options = []\n","        self.rewards = []\n","\n","    def update(self, episode):\n","        trajectory = self.memory[episode]\n","        states = trajectory[0]\n","        options = trajectory[1]\n","        rewards = trajectory[2]\n","\n","        T = len(states)\n","\n","        returns = np.zeros(T)\n","        returns[-1] = rewards[-1]\n","        for t in reversed(range(1, T)):\n","            returns[t - 1] = rewards[t - 1] + self.discount_rate * returns[t]\n","\n","        x = np.reshape(states, (1, T, 7))\n","        y = np.zeros((1, T, 7))\n","\n","        for i in range(T):\n","            y[0, i, options[i]] = returns[i]\n","\n","        self.model.fit(x, y, epochs=1, verbose=0)\n","    \n","    def batch_update(self, start, end):\n","        for episode in range(start, end):\n","            self.update(episode)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2YMrMbtwNoAt"},"source":["def select_action(state, option):\n","    return 1 if np.argmax(state) < option else 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t9aP2Te2RnF5"},"source":["def train(start, runs):\n","    if start == 0:\n","        rewards = np.zeros((runs, 10000))\n","    else:\n","        rewards = np.load(f\"./drive/My Drive/corridor/mnemonic_rewards.npy\")\n","\n","    for run in range(start, runs):\n","        print(\"\\nRun \" + str(run))\n","\n","        env = Corridor()\n","        meta = RecurrentHReinforce()\n","\n","        for episode in range(10000):\n","            # print(\"\\nEpisode \" + str(episode))\n","\n","            done = False\n","            meta_state = env.reset()\n","            episode_reward = 0\n","\n","            state_list = []\n","            state_list.append(meta_state)\n","\n","            while not done:\n","                if episode < 1000:\n","                    applicable = np.delete(np.arange(7), np.argmax(meta_state))\n","                    option = np.random.choice(applicable)\n","                else:\n","                    option = meta.select_option(state_list)\n","                reached = np.argmax(meta_state) == option\n","                \n","                option_reward = 0\n","\n","                state = meta_state\n","                while not done and not reached:\n","                    action = select_action(state, option)\n","                    next_state, reward, done = env.step(action)\n","                    \n","                    option_reward += reward\n","                    episode_reward += reward\n","                    \n","                    reached = np.argmax(next_state) == option\n","                    state = next_state\n","                \n","                meta.store_transition(meta_state, option, option_reward)\n","                meta_state = state\n","                state_list.append(meta_state)\n","\n","            meta.store_episode()\n","            meta.update(episode)\n","\n","            rewards[run, episode] = episode_reward\n","\n","            # print(\"reward =  \" + str(episode_reward))\n","\n","            if episode % 1000 == 999:\n","                np.save(f\"./drive/My Drive/corridor/mnemonic_rewards\", rewards)\n","        \n","        meta.model.save(f\"./drive/My Drive/corridor/mnemonic_{run}_{episode}.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5WVYUp7aRn0b"},"source":["train(0, 10)"],"execution_count":null,"outputs":[]}]}